{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jha-nishant/ai-practice/blob/main/ProgrammaticPromptingEncoded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install litellm\n",
        "\n",
        "# Important!!!\n",
        "#\n",
        "# <---- Set your 'OPENAI_API_KEY' as a secret over there with the \"key\" icon\n",
        "#\n",
        "#\n",
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "KEYrzG2vB8Ip"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwe2eeOQB0cC",
        "outputId": "a5747c60-0c41-4729-be70-a8c6778867f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'SGVyZSdzIGEgc2ltcGxlIGZ1bmN0aW9uYWwtc3R5bGUgUHl0aG9uIGZ1bmN0aW9uIHRvIHN3YXAgdGhlIGtleXMgYW5kIHZhbHVlcyBpbiBhIGRpY3Rpb25hcnk6CgpgYGBweXRob24KZGVmIHN3YXBfZGljdF9rZXlzX3ZhbHVlcyhkKToKICAgIHJldHVybiB7djogayBmb3IgaywgdiBpbiBkLml0ZW1zKCl9CmBgYAoKVXNhZ2UgZXhhbXBsZToKCmBgYHB5dGhvbgpvcmlnaW5hbCA9IHsnYSc6IDEsICdiJzogMiwgJ2MnOiAzfQpzd2FwcGVkID0gc3dhcF9kaWN0X2tleXNfdmFsdWVzKG9yaWdpbmFsKQpwcmludChzd2FwcGVkKSAgIyBPdXRwdXQ6IHsxOiAnYScsIDI6ICdiJywgMzogJ2MnfQpgYGAKClRoaXMgZnVuY3Rpb24gdXNlcyBhIGRpY3Rpb25hcnkgY29tcHJlaGVuc2lvbiwgd2hpY2ggaXMgYSBmdW5jdGlvbmFsIHN0eWxlIGNvbnN0cnVjdCwgdG8gY3JlYXRlIGEgbmV3IGRpY3Rpb25hcnkgd2l0aCBrZXlzIGFuZCB2YWx1ZXMgc3dhcHBlZC4gSXQgYXNzdW1lcyB0aGF0IHRoZSB2YWx1ZXMgaW4gdGhlIG9yaWdpbmFsIGRpY3Rpb25hcnkgYXJlIHVuaXF1ZSBhbmQgaGFzaGFibGUu'\n"
          ]
        }
      ],
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "import base64\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4.1-mini\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "\n",
        "    resp = response.choices[0].message.content\n",
        "    resp_bytes = resp.encode(\"ascii\")\n",
        "    base64_bytes = base64.b64encode(resp_bytes)\n",
        "    return base64_bytes\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    }
  ]
}